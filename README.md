<h2><b>Simple CNN with Tensorflow</b></h2>

<h3>Convolutional neural networks that recognize numbers in 28x28 pixel images from the mnist.dataset</h3>
</br>

<p><img src = "images/0001.png"></p>


<h3>Activation functions implemented:</h3>
<ol>
      <li>ReLU</li>
      <li>Softmax</li>
</ol>

<h3>ReLU - Rectified Linear Unit</h3>

<h5><p align='Justify'>This activation function started showing up in the context of visual feature extraction in hierarchical neural networks starting in the late 1960s. It was later argued that it has strong biological motivations and mathematical justifications.In 2011 it was found to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, as of 2017, the most popular activation function for deep neural networks. <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target ="_blank">[wikipedia]</a></p></h5>
<h5><i>"The ReLU will output the input directly if it is positive, otherwise, it will output zero".<a href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/#:~:text=The%20rectified%20linear%20activation%20function,otherwise%2C%20it%20will%20output%20zero.">[machinelearningmastery]</a></i></h5>

</br>

<p><img src = "images/ReLu Function.png"></p>
